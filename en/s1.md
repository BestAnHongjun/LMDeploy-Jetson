# S1.Quantize on server by W4A16

The LLMs occupy a large amount of GPU memory during inference. We can use the LMDeploy tool to quantize the model to [W4A16](https://github.com/InternLM/lmdeploy/blob/main/docs/en/quantization/w4a16.md) format and convert it into a [TurboMind](https://github.com/InternLM/lmdeploy/blob/main/docs/en/inference/turbomind.md) model. This can significantly reduce GPU memory usage, enabling the deployment of LLMs on the Jetson edge computing platform.

### 1.Setup conda environment

The installation method for Anaconda is omitted.

Setup conda environment：

```sh
conda create -n lmdeploy python=3.10
```

Activate conda environment：

```sh
conda activate lmdeploy
```

### 2.Install LMDeploy

Install lmdeploy by pip.

```sh
# ref：https://github.com/InternLM/lmdeploy/issues/1169
pip install torch==2.1.2 torchvision==0.16.2 torchaudio==2.1.2 --index-url https://download.pytorch.org/whl/cu118
pip install lmdeploy[all]==0.2.3
```

### 3.Download HF model

```sh
mkdir -p ~/models && cd ~/models
```

Install dependencies.

```sh
pip install modelscope
```

Create file `download_models.py`：

```py
from modelscope import snapshot_download
model_dir = snapshot_download('Shanghai_AI_Laboratory/internlm-chat-7b', cache_dir='internlm-chat-7b')
print(model_dir)
```

> `internlm-chat-7b`` can be replaced with different models, such as `internlm-chat-20b`,`internlm2-chat-1_8b`,`internlm2-chat-7b`,`internlm2-chat-20b`.

Run the download script.

```sh
python download_models.py
```

The final printed output path is the path where the model is saved. Please make a note of it. 

```sh
internlm-chat-7b/Shanghai_AI_Laboratory/internlm-chat-7b
```

### 4.Quantize model by W4A16

```sh
export HF_MODEL=./internlm-chat-7b/Shanghai_AI_Laboratory/internlm-chat-7b
export WORK_DIR=./internlm-chat-7b-4bit

lmdeploy lite auto_awq \
   $HF_MODEL \
  --calib-dataset 'ptb' \
  --calib-samples 128 \
  --calib-seqlen 2048 \
  --w-bits 4 \
  --w-group-size 128 \
  --work-dir $WORK_DIR
```

Convert model format.

```sh
export TM_DIR=./internlm-chat-7b-turbomind

lmdeploy convert  internlm-chat-7b \
    $WORK_DIR \
    --model-format awq \
    --group-size 128 \
    --dst-path $TM_DIR
```

Open the configuration file `internlm-chat-7b-turbomind/triton_models/weights/config.ini` and modify the following three lines.

```ini
cache_max_entry_count = 0.5
cache_block_seq_len = 128
cache_chunk_size = 1
```

Compress the TurboMind model.

```sh
apt-get install pigz    # Multi-threads speed up
tar --use-compress-program=pigz -cvf internlm-chat-7b-turbomind.tgz ./internlm-chat-7b-turbomind 
```

Keep `internlm-chat-7b-turbomind.tgz``, it will be used later.