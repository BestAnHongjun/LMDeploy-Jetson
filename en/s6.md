# S6.Port LMDeploy-2.3.0 to Jetson

Download lmdeploy-v0.2.3：

```sh
cd ~/
git clone https://github.com/InternLM/lmdeploy.git
cd lmdeploy 
git checkout 2831dc2 # 确保为0.2.3版本
```

Activate conda environment：

```sh
conda activate lmdeploy
```

Install dependencies.

```sh
cd ~/lmdeploy
pip install -r requirements/build.txt
```

Create a new file named `generate_jetson.sh` under `~/lmdeploy`, and fill in the following content:

```sh
#!/bin/sh

builder="-G Ninja"

if [ "$1" == "make" ]; then
    builder=""
fi

cmake ${builder} .. \
    -DCMAKE_BUILD_TYPE=RelWithDebInfo \
    -DCMAKE_EXPORT_COMPILE_COMMANDS=1 \
    -DCMAKE_INSTALL_PREFIX=./install \
    -DBUILD_PY_FFI=ON \
    -DBUILD_MULTI_GPU=OFF \
    -DCMAKE_CUDA_FLAGS="-lineinfo" \
    -DUSE_NVTX=ON

```

Modify file permissions.

```sh
chmod +x generate_jetson.sh
```

Install Ninja.

```sh
sudo apt-get install ninja-build
```

Create a new build folder.

```sh
cd ~/lmdeploy
mkdir build && cd build
```

Compile LMDeploy.

```sh
../generate_jetson.sh
ninja install
```

Use vim to edit `requirements/runtime.txt`, and delete the lines containing `torch<=2.1.2,>=2.0.0` and `triton>=2.1.0,<2.2.0`.

**Note**: To simplify dependencies, we have removed `triton`. This also means that when deploying models using lmdeploy, they can only be invoked through the turbomind method, and not through the API method.

Install lmdeploy-v0.2.3 locally.

```sh
cd ~/lmdeploy
pip install -e .[serve]
```