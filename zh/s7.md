# S7.Jetson端离线运行InternLM大模型

创建模型保存目录：

```sh
mkdir -p ~/models
```

将[S1.服务器端模型W4A16量化](./s1.md)得到的`internlm-chat-7b-turbomind.tgz`上传到`models`目录下。

解压模型文件：

```sh
tar zxvf internlm-chat-7b-turbomind.tgz -C .
```

### 1.终端运行

激活conda环境：

```sh
conda activate lmdeploy
```

运行模型：

```sh
lmdeploy chat turbomind ./internlm-chat-7b-turbomind
```

![](../attach/cli.jpg)

### 2.Python集成运行

编写运行脚本`run_model.py`，内容如下：

```py
from lmdeploy import turbomind as tm


if __name__ == "__main__":
    model_path = "./internlm-chat-7b-turbomind" # 修改成你的路径

    tm_model = tm.TurboMind.from_pretrained(model_path)
    generator = tm_model.create_instance()

    while True:
        inp = input("[User] >>> ")
        if inp == "exit":
            break
        prompt = tm_model.model.get_prompt(inp)
        input_ids = tm_model.tokenizer.encode(prompt)
        for outputs in generator.stream_infer(session_id=0, input_ids=[input_ids]):
            res = outputs[1]
        response = tm_model.tokenizer.decode(res)
        print("[Bot] <<< {}".format(response))

```

激活conda环境：

```sh
conda activate lmdeploy
```

运行脚本：

```sh
python run_model.py
```

![](../attach/python.jpg)
