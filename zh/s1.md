# S1.服务器端模型W4A16量化

大模型推理时占用显存巨大，我们可以借助LMDeploy工具对模型进行[W4A16量化](https://github.com/InternLM/lmdeploy/blob/main/docs/zh_cn/quantization/w4a16.md)，转换为[TurboMind](https://github.com/InternLM/lmdeploy/blob/main/docs/zh_cn/inference/turbomind.md)模型，这样在推理时可以极大减少显存占用，使得在Jetson边缘计算平台部署大模型成为可能。

### 1.创建conda环境

安装Anaconda方法略。

创建conda环境：

```sh
conda create -n lmdeploy python=3.10
```

激活conda环境：

```sh
conda activate lmdeploy
```

### 2.安装LMDeploy

使用pip方法安装lmdeploy。

```sh
# 直接用pip装lmdeploy时安装的pytorch的cuda可能是12版本的，运行时会引发链接错误
# ref：https://github.com/InternLM/lmdeploy/issues/1169
pip install torch==2.1.2 torchvision==0.16.2 torchaudio==2.1.2 --index-url https://download.pytorch.org/whl/cu118
# 安装好torch-2.1.2-cu118后再安装lmdeploy
pip install lmdeploy[all]==0.2.3
```

### 3.下载HF模型权重文件

创建目录

```sh
mkdir -p ~/models && cd ~/models
```

安装依赖项。

```sh
pip install modelscope
```

创建Python文件`download_models.py`：

```py
#模型下载
from modelscope import snapshot_download
model_dir = snapshot_download('Shanghai_AI_Laboratory/internlm-chat-7b', cache_dir='internlm-chat-7b')
print(model_dir)
```

> 其中，internlm-chat-7b可替换为不同的模型，比如`internlm-chat-20b`，`internlm2-chat-1_8b`，`internlm2-chat-7b`，`internlm2-chat-20b`，下同。

运行下载脚本：

```sh
python download_models.py
```

最后打印输出的路径就是模型保存的路径，请记录下。笔者为：

```sh
internlm-chat-7b/Shanghai_AI_Laboratory/internlm-chat-7b
```

### 4.模型W4A16量化

```sh
export HF_MODEL=./internlm-chat-7b/Shanghai_AI_Laboratory/internlm-chat-7b
export WORK_DIR=./internlm-chat-7b-4bit

lmdeploy lite auto_awq \
   $HF_MODEL \
  --calib-dataset 'ptb' \
  --calib-samples 128 \
  --calib-seqlen 2048 \
  --w-bits 4 \
  --w-group-size 128 \
  --work-dir $WORK_DIR
```

转换模型格式。

```sh
export TM_DIR=./internlm-chat-7b-turbomind

lmdeploy convert  internlm-chat-7b \
    $WORK_DIR \
    --model-format awq \
    --group-size 128 \
    --dst-path $TM_DIR
```

修改配置文件`internlm-chat-7b-turbomind/triton_models/weights/config.ini`，修改如下三行：

```ini
cache_max_entry_count = 0.5
cache_block_seq_len = 128
cache_chunk_size = 1
```

压缩turbomind模型。

```sh
apt-get install pigz    # 多线程加速压缩
tar --use-compress-program=pigz -cvf internlm-chat-7b-turbomind.tgz ./internlm-chat-7b-turbomind 
```

将`internlm-chat-7b-turbomind.tgz`保留备用。